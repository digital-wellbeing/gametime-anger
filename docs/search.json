[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"repository (GitHub / OSF) contains data code required reproduce analyses reported manuscript, Time spent playing two online shooters measurable effect aggressive affect (Johannes, Vuorre, Magnusson, & Przybylski, 2022). analyses presented Online analysis supplement.","code":""},{"path":"index.html","id":"materials","chapter":"1 Introduction","heading":"1.1 Materials","text":"Preprint\npublicly available version manuscript advance peer-review formal publication\npublicly available version manuscript advance peer-review formal publicationGitHub repository\nversion controlled repository containing raw data code project\nversion controlled repository containing raw data code projectOSF repository\narchived permanent copy GitHub repository well study materials.\narchived permanent copy GitHub repository well study materials.Online analysis supplement\noutput document analyses, rendered website.\noutput document analyses, rendered website.Note: project uses data code previous study: Vuorre et al. (2021). code thus mostly taken project, adjustments current study.","code":""},{"path":"index.html","id":"reproducibility","chapter":"1 Introduction","heading":"1.2 Reproducibility","text":"raw data data/ directory repository. code used clean analyze data organized R Markdown (.Rmd) files directory, meant run sequence indicated numeric prefixes. project uses private package library help renv package. Therefore, first download entire project, install package (install.packages(\"renv\")), run renv::restore(). run cleaning analyses, compile resulting document (online analysis supplement), run bookdown::render_book() R click “Build Book” RStudio IDE.","code":""},{"path":"data-processing.html","id":"data-processing","chapter":"2 Data processing","heading":"2 Data processing","text":"’ll use data Vuorre et al (2021).\nsurvey data telemetry (.e., video game data) seven games.\nHowever, rather using full data set, ’ll analyze two games (Apex Outriders) one item well-scale (namely SPANE), namely item asks angry someone felt past two weeks.data already processed authors : https://digital-wellbeing.github.io/gametime-longitudinal/ However, processing results cleaned data set doesn’t individual SPANE items anymore.\nTherefore, ’ll redo processing, slight adjustments get play two games feeling angry, variables.Anything thus blatant copy code used Vuorre et al, minor adjustments.\nFirst: packages.\nproject private library renv package.\nSee instructions Readme file recreate private library.data sets quite large, set options parallel computing well options plots.","code":"\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(here)\nlibrary(scales)\nlibrary(lubridate)\nlibrary(gtsummary)\nlibrary(multidplyr)\nlibrary(showtext)\nlibrary(tidyverse)\nlibrary(sessioninfo)\nlibrary(extrafont)\n# Plotting options\nFont <- \"Titillium Web\"\nfont_add_google(Font, Font)\ntheme_set(\n  theme_linedraw(\n    base_family = Font,\n    base_size = 12\n  ) +\n    theme(\n      panel.grid.minor = element_blank(),\n      panel.grid.major.x = element_blank()\n    )\n)\n\n# parallel computations\nMAX_CORES <- as.numeric(Sys.getenv(\"MAX_CORES\"))\nif (is.na(MAX_CORES)) MAX_CORES <- parallel::detectCores(logical = FALSE)\ncluster <- new_cluster(MAX_CORES)\n# load packages on clusters\ncluster_library(cluster, c(\"dplyr\", \"lubridate\"))\n\n# For saving intermediate files\nif (!dir.exists(here(\"temp\"))) {\n  dir.create(\"temp\", FALSE)\n}"},{"path":"data-processing.html","id":"get-data","chapter":"2 Data processing","heading":"2.1 Get data","text":"Next, download three files need straight form OSF page: survey data telemetry two games.","code":"\nif (!dir.exists(here(\"data\")) || length(list.files(here(\"data\"))) == 0 ){\n  dir.create(\"data/\", FALSE, TRUE)\n  \n  download.file(\"https://osf.io/h87sb/download\", here(\"data\", \"qualtrics.csv.gz\"), mode = \"wb\" )\n  download.file(\"https://osf.io/c2e93/download\", here(\"data\", \"telemetry_apex_legends.csv.gz\"), mode = \"wb\")\n  download.file(\"https://osf.io/xam5t/download\", here(\"data\", \"telemetry_outriders.csv.gz\"), mode = \"wb\")\n}"},{"path":"data-processing.html","id":"process-survey-data","chapter":"2 Data processing","heading":"2.2 Process survey data","text":"Next, process raw Qualtrics file.\n, almost everything identical original code, minor adjustments select variables need.Next, like original paper, exclude anyone doesn’t survey responses variables need.\ncase, ’s fewer variables require, can use responses.\nOverall, ’s people don’t usable responses.\nNote: slight preprocessing preserve participants’ privacy.\nmeans current sample includes constented participate, meaning top row tells us initial sample size.\nTable 2.1: Summary participants without responses.\n’ll remove without survey responses.Next: interval waves wasn’t always exactly two weeks.\ncompanies sent reminders 13 days respondents took .\nreasons, interval waves variable, calculate .","code":"\n# read_csv() automatically decompresses the .gz archive\nd <- read_csv(here(\"data\", \"qualtrics.csv.gz\"))\n\n# Clean responses to the question asking if they played in past 2 weeks\nd <- d %>%\n  mutate(played = factor(!str_detect(played, \"NOT\")))\n\n# Create estimated time played variable from reported hours & mins\nd <- d %>%\n  mutate(minutes = minutes / 60) %>%\n  mutate(\n    hours_est = rowSums(select(., hours, minutes), na.rm = TRUE)\n  ) %>%\n  # sum above returns 0 if both hours and minutes are NA, fix here:\n  mutate(\n    hours_est = if_else(is.na(hours) & is.na(minutes), NaN, hours_est)\n  ) %>%\n  select(-minutes, -hours)\n\n# Ensure correct ordering and variable type of item responses\nspane_levels <- c(\n  \"Very rarely or never\",\n  \"Rarely\",\n  \"Occasionally\",\n  \"Sometimes\",\n  \"Frequently\",\n  \"Often\",\n  \"Very often or always\"\n)\npens_levels <- c(\n  \"Strongly disagree\",\n  \"Disagree\",\n  \"Somewhat disagree\",\n  \"Neither agree nor disagree\",\n  \"Somewhat agree\",\n  \"Agree\",\n  \"Strongly agree\"\n)\n\nd <- d %>%\n  mutate(\n    across(\n      starts_with(\"spane_\"),\n      function(x) {\n        factor(\n          x,\n          levels = spane_levels\n        )\n      }\n    )\n  )\nd <- d %>%\n  mutate(\n    across(\n      starts_with(\"pens_\"),\n      function(x) {\n        factor(\n          x,\n          levels = pens_levels\n        )\n      }\n    )\n  )\n\n# Convert item responses to numbers\nd <- d %>%\n  mutate(\n    across(\n      c(starts_with(\"spane_\"), starts_with(\"pens_\")),\n      as.numeric\n    )\n  )\n\n# Reverse reverse-scored items\nreverse_items <- c(\n  \"pens_needs_9\",\n  \"pens_motivations_2\",\n  \"pens_motivations_3\"\n)\n\nd <- d %>%\n  mutate(\n    across(all_of(reverse_items), ~ 8 - .x)\n  )\n\n# Subscale items\nspane_pos_items <- paste0(\"spane_\", c(1, 3, 5, 7, 10, 12))\nspane_neg_items <- paste0(\"spane_\", c(2, 4, 6, 8, 9, 11))\nautonomy_items <- paste0(\"pens_needs_\", 1:3)\ncompetence_items <- paste0(\"pens_needs_\", 4:6)\nrelatedness_items <- paste0(\"pens_needs_\", 7:9)\nintrinsic_items <- paste0(\"pens_motivations_\", 1:4)\nextrinsic_items <- paste0(\"pens_motivations_\", 5:8)\n\n# Create (sub)scale scores (means of item responses)\nd <- d %>%\n  mutate(\n    spane_pos = rowMeans(\n      select(., all_of(spane_pos_items)),\n      na.rm = TRUE\n    ),\n    spane_neg = rowMeans(\n      select(., all_of(spane_neg_items)),\n      na.rm = TRUE\n    ),\n    spane = spane_pos - spane_neg,\n    intrinsic = rowMeans(\n      select(., all_of(intrinsic_items)),\n      na.rm = TRUE\n    ),\n    extrinsic = rowMeans(\n      select(., all_of(extrinsic_items)),\n      na.rm = TRUE\n    ),\n    autonomy = rowMeans(\n      select(., all_of(autonomy_items)),\n      na.rm = TRUE\n    ),\n    competence = rowMeans(\n      select(., all_of(competence_items)),\n      na.rm = TRUE\n    ),\n    relatedness = rowMeans(\n      select(., all_of(relatedness_items)),\n      na.rm = TRUE\n    ),\n  )\n\n# Then remove and rename variables, except for the angry item\nd <- d %>%\n  select(\n    -all_of(\n      c(\n        spane_pos_items,\n        spane_neg_items,\n        autonomy_items,\n        competence_items,\n        relatedness_items,\n        intrinsic_items,\n        extrinsic_items\n      )\n    ),\n    spane_11\n  )\n\n# Gender as factor\nd <- d %>%\n  mutate(gender = factor(gender))\n\n# Prettier names for tables/figures\nd <- d %>%\n  rename(\n    Angry = spane_11\n  ) %>%\n  rename_with(\n    str_to_title,\n    c(played:experience, game, company, intrinsic:relatedness)\n  )\n\n# Make table easier to look at by including only variables we need\n# in a reasonable order\nd <- d %>%\n  select(\n    Game, pid, wid,\n    Angry,\n    Intrinsic, Extrinsic, hours_est,\n    StartDate, Age, Gender, Experience\n  ) %>%\n  arrange(Game, pid, wid)\n\n# only keep relevant games\nd <- \n  d %>% \n  filter(Game %in% c(\"Apex Legends\", \"Outriders\"))\n# Person-wave level indicator if person answered to the DV of interest for that wave\nd$Responded <- apply(\n  select(d, Angry), 1,\n  function(x) sum(!is.na(x)) > 0\n)\n\n# Person-level indicator of how many waves responded to angry item\nd <- d %>%\n  group_by(Game, pid) %>%\n  mutate(\n    `# of waves with response` = sum(Responded),\n    `Any waves with response` = factor(`# of waves with response` > 0)\n  ) %>%\n  ungroup()\n\n# Table of waves answered to by game\nd %>%\n  distinct(\n    Game, pid,\n    `# of waves with response`,\n    `Any waves with response`\n  ) %>%\n  select(-pid) %>%\n  tbl_summary(by = Game) %>%\n  add_overall() %>%\n  as_kable_extra(\n    caption = \"Summary of participants with and without responses.\"\n  ) %>% \n  kable_styling(full_width = FALSE, font_size = 12)\n# Take out all who didn't answer a single wave\nd <- filter(d, `Any waves with response` == \"TRUE\")\n\n# Remove the indicators\nd <- select(d, -`# of waves with response`, -`Any waves with response`)\nsurvey_intervals <- d %>%\n  select(Game, pid, wid, StartDate) %>%\n  arrange(pid, wid) %>%\n  # Make sure that there is a row for each subject X wave\n  # so interval is calculated correctly\n  complete(wid, nesting(pid, Game)) %>%\n  arrange(pid, wid) %>%\n  group_by(pid) %>%\n  partition(cluster) %>%\n  # Interval between waves in days\n  mutate(\n    interval = (as.numeric(StartDate) - as.numeric(lag(StartDate))) /\n      3600 / 24\n  ) %>%\n  collect() %>%\n  ungroup() %>%\n  select(wid, pid, Game, interval)\nd <- left_join(d, survey_intervals)"},{"path":"data-processing.html","id":"process-telemetry","chapter":"2 Data processing","heading":"2.3 Process telemetry","text":"original processing: files minimally processed versions ones received publishers.\n(Players didn’t explicitly consent survey excluded, variable names harmonised, tables reshaped format.) , load telemetry two games interest.Next, calculate many hours player played session.Following original, check many implausible values sessions(.e., sessions outside two week interval, negative durations, sessions longer 10 hours).\nApex doesn’t many bad sessions; Outriders quite lot.\nTable 2.2: Summaries raw session durations\nLet’s remove sessions keep ones can use analysis.Sometimes sessions overlap.\ncase, merge one longer sessions.\nVuorre et al function can use .merging overlapping sessions.\nOny laptop, took around 20 minutes.Now merged intervals removed overlap, ’ll replace original intervals updated one.\nAlso, merging created empty rows (aka rows NA) - rows merged now redundant, drop .Last, merging might’ve new sessions created longer 10h, exclude .","code":"\n# Apex Legends\nt_al <- read_csv(here(\"data\", \"telemetry_apex_legends.csv.gz\"))\n\n# Select relevant variables\nt_al <- t_al %>%\n  select(\n    pid, session_start, session_end\n  ) %>%\n  # Format datetimes\n  transmute(\n    pid,\n    session_start = as_datetime(mdy_hm(session_start), tz = \"UTC\"),\n    session_end = as_datetime(mdy_hm(session_end), tz = \"UTC\"),\n    Game = \"Apex Legends\"\n  )\n\n# Outriders\nt_or <- read_csv(here(\"Data\", \"telemetry_outriders.csv.gz\"))\n\n# Select relevant variables\nt_or <- t_or %>%\n  select(pid, session_start, session_end) %>%\n  mutate(Game = \"Outriders\")\n\n# combine the two\nd_t <- bind_rows(\n  t_al, t_or\n)\n\nremove(t_al, t_or)\nd_t <- d_t %>%\n  mutate(\n    interval = interval(session_start, session_end)\n  ) %>%\n  mutate(Hours = as.numeric(as.duration(interval)) / 3600)\n# Create indicators for implausible timestamps\nd_t <- d_t %>%\n  mutate(\n    `Session under 0h` = Hours < 0,\n    `Session over 10h` = Hours > 10,\n    `Session before` = session_end < min(d$StartDate) - days(14),\n    `Session after` = session_start > max(d$StartDate)\n  )\n\n# Show a table of raw sessions and potential bad sessions\nd_t %>%\n  select(Game, Hours, starts_with(\"Session \")) %>%\n  tbl_summary(\n    by = Game,\n    statistic = list(all_continuous() ~ \"{median} ({min}, {max})\")\n  ) %>%\n  add_overall() %>%\n  as_kable_extra(caption = \"Summaries of raw session durations\") %>% \n  kable_styling(full_width = FALSE, font_size = 12)\n# Then remove flagged sessions from data\nd_t <- d_t %>%\n  filter(\n    between(Hours, 0, 10),\n    !`Session before`,\n    !`Session after`\n  )\n\n# And now unnecessary variables\nd_t <- d_t %>% \n  select(-starts_with(\"Session \"))\nif (!file.exists(here(\"merge_intervals.R\"))){\n  download.file(\"https://osf.io/najbr/download\", here(\"merge_intervals.R\"), mode = \"wb\" )\n}\n\nsource(here(\"merge_intervals.R\"))\n# explicitly cache\ndata_path <- here(\"temp\", \"session-overlap-merged.rds\")\nif (file.exists(data_path)) {\n  message(\"Loading cached data\")\n  d_t <- read_rds(file = data_path)\n} else {\n  message(\n    \"Merging overlapping sessions (grab a coffee, this will take a while)\"\n  )\n  cluster_copy(cluster, c(\"merge_interval\", \"merge_intervals_all\"))\n  d_t <- d_t %>%\n    group_by(pid, Game) %>%\n    partition(cluster) %>%\n    mutate(\n      interval = interval(session_start, session_end)\n    ) %>%\n    arrange(session_start, session_end, .by_group = TRUE) %>%\n    mutate(interval_merged = merge_intervals_all(interval)) %>%\n    collect() %>%\n    ungroup()\n  write_rds(d_t, file = data_path)\n}\nd_t <- d_t %>% \n  select(-interval) %>% \n  rename(interval = interval_merged) %>% \n  drop_na(interval) %>% \n  select(Game, pid, interval)\nd_t <- d_t %>% \n  filter(as.numeric(as.duration(interval))/3600 <= 10)"},{"path":"data-processing.html","id":"merge-survey-and-telemetry","chapter":"2 Data processing","heading":"2.4 Merge survey and telemetry","text":", merge surveys telemetry.\nFirst, someone might telemetry wave didn’t respond.\nTherefore, make sure data combination participant ID wave number.Now add start session survey (aka wave) telemetry anchor aggregate play past two weeks.filter sessions happened outside two-week time window preceding survey.Now session might end beginning window duration, need cut (aka partial vs. complete retains).Let’s aggregate sessions total playtime (plus number sessions) two-week window per wave.Nice, now can add survey data.","code":"\n# Complete data for all pid-wid combinations (all pids have 3 rows; new rows have NAs for all other variables)\nd <- d %>%\n  complete(nesting(Game, pid), wid)\n\n# If a survey wasn't responded to, replace start date with previous wave's date + two weeks. Enables creating a two-week window preceding \"survey response\" to count hours played.\nd <- d %>%\n  arrange(Game, pid, wid) %>%\n  group_by(Game, pid) %>%\n  partition(cluster) %>%\n  # Fill potential missing wave 2 with wave 1 + 14\n  mutate(\n    StartDate = if_else(\n      is.na(StartDate),\n      lag(StartDate, 1) + days(14),\n      StartDate\n    )\n  ) %>%\n  # Fill potential missing wave 3 with wave 2 + 14\n  mutate(\n    StartDate = if_else(\n      is.na(StartDate),\n      lag(StartDate, 1) + days(14),\n      StartDate\n    )\n  ) %>%\n  collect() %>%\n  ungroup()\nd_t <- d %>%\n  select(Game, pid, wid, StartDate) %>%\n  left_join(d_t)\n# Then keep only those sessions that were in that wave's time window:\n# Is session start and/or end within 0-2 weeks preceding survey?\nd_t <- d_t %>%\n  mutate(\n    start_in = int_start(interval) %within%\n      interval(StartDate - days(14), StartDate),\n    end_in = int_end(interval) %within%\n      interval(StartDate - days(14), StartDate)\n  )\nd_t <- d_t %>%\n  filter(start_in | end_in)\n# Exact duration depends on if session was completely in window or partially\nd_t <- d_t %>%\n  mutate(\n    Hours = case_when(\n      # Entire session in window: All time counts\n      start_in & end_in ~ as.duration(interval),\n      # Started before window, ended within: start of window to end of session\n      !start_in & end_in ~ as.duration(\n        int_end(interval) - (StartDate - days(14))\n      ),\n      # Started in window, ended after: Session start to end of window\n      start_in & !end_in ~ as.duration(StartDate - int_start(interval))\n    )\n  ) %>%\n  mutate(Hours = as.numeric(Hours) / 3600)\n# Summarise per wave to sum hours and number of sessions\n# this also sets sum hours to zero for people with no telemetry\nd_t <- d_t %>%\n  group_by(Game, pid, wid) %>%\n  summarise(\n    Sessions = sum(!is.na(Hours)),\n    Hours = sum(Hours, na.rm = TRUE) # is 0 if all Hours are NA\n  ) %>%\n  ungroup()\n# Join back to survey data\nd <- left_join(d, d_t)\n\n# This creates NA hours for people who didn't exist in telemetry,\n# thus we can replace NAs with zeros.\nd <- d %>%\n  mutate(Hours = replace_na(Hours, 0))"},{"path":"data-processing.html","id":"exclusions","chapter":"2 Data processing","heading":"2.5 Exclusions","text":"Following Vuorre et al., include participants analysis actually telemetry - otherwise ’s point analyzing data.\nTable 2.3: Summary participants without responses.\nTelemetry measurement error.\nTherefore, exlcude entries unrealistically high.\nFollowing Vuorre et al, set entries missing result 16h play per day.\nSimilarly, someone estimates daily play 16h higher, can’t fully trust subjective estimates play.\nsee nobody telemetry high negligible number participants estimated high play volumes.\nTable 2.4: Numbers (%) person-waves 16h/day play\nLast, ’s theoretically possible participants wave 3 wave 2 (ignoring invite first, going back).\nTherefore, exclude id-wave combinations negative interval.\ninterval NA wave wasn’t completed, resulting unknown values table .\nLuckily negative intervals.\nTable 2.5: Numbers (%) person-waves negative intervals\nfollows, ’ll work hourly play per day.\nTherefore, need divide time estimates 14.Turn wave nicely labelled factor.","code":"\n# Indicator if person played at wave\nd <- d %>%\n  mutate(Played = Hours > 0)\n\n# Create participant-level indicator of whether there was any telemetry\nd <- d %>%\n  group_by(Game, pid) %>%\n  mutate(\n    `# of waves with play` = sum(Played),\n    `Any waves with play` = factor(`# of waves with play` > 0)\n  ) %>%\n  ungroup()\n\n# Table of waves with play by game\nd %>%\n  distinct(\n    Game, pid,\n    `# of waves with play`,\n    `Any waves with play`\n  ) %>%\n  select(-pid) %>%\n  tbl_summary(by = Game) %>%\n  add_overall() %>%\n  as_kable_extra(\n    caption = \"Summary of participants with and without responses.\"\n  ) %>% \n  kable_styling(full_width = FALSE, font_size = 12)\n# Take out all who didn't have any telemetry\nd <- filter(d, `Any waves with play` == \"TRUE\")\n\n# Remove the indicators\nd <- select(d, -`# of waves with play`, -`Any waves with play`)\nd %>%\n  mutate(Over_16h_day_telemetry = Hours / 14 > 16) %>%\n  mutate(Over_16h_day_subjective = hours_est / 14 > 16) %>%\n  select(Game, starts_with(\"Over_\")) %>%\n  tbl_summary(by = Game) %>%\n  add_overall() %>%\n  as_kable_extra(\n    caption = \"Numbers (%) of person-waves with more than 16h/day of play\"\n  ) %>% \n  kable_styling(full_width = FALSE, font_size = 12)\nd <- d %>%\n  mutate(\n    Hours = if_else(Hours / 14 > 8, NaN, Hours),\n    hours_est = if_else(hours_est / 14 > 8, NaN, hours_est)\n  )\nd %>%\n  mutate(Negative_interval = interval < 0) %>%\n  select(Game, Negative_interval) %>%\n  tbl_summary(by = Game) %>%\n  add_overall() %>%\n  as_kable_extra(\n    caption = \"Numbers (%) of person-waves with negative intervals\"\n  ) %>% \n  kable_styling(full_width = FALSE, font_size = 12)\nd <- \n  d %>% \n  mutate(\n    across(\n      c(Hours, hours_est),\n      ~ .x / 14\n    )\n  )\n# Make wave a nicely labelled factor\nd <- d %>%\n  mutate(Wave = factor(wid, levels = 1:3, labels = paste0(\"Wave \", 1:3)))"},{"path":"data-processing.html","id":"save-cleaned-data","chapter":"2 Data processing","heading":"2.6 Save cleaned data","text":"Last, save cleaned data file.","code":"\nwrite_rds(d, file = here(\"data\", \"cleaned_data.rds\"), compress = \"gz\")"},{"path":"descriptives.html","id":"descriptives","chapter":"3 Descriptives","heading":"3 Descriptives","text":"Load packages.\n: code almost one--one copied Vuorre et al.Set figure options like .Next, load cleaned data set previous section.","code":"\nlibrary(knitr)\nlibrary(scales)\nlibrary(gtsummary)\nlibrary(kableExtra)\nlibrary(here)\nlibrary(showtext)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggdist)\nlibrary(ggstance)\nlibrary(extrafont)\nlibrary(janitor)\nlibrary(broom.mixed)\nlibrary(ggbeeswarm)\n# Plotting options\nFont <- \"Titillium Web\"\nfont_add_google(Font, Font)\ntheme_set(\n  theme_linedraw(\n    base_family = Font,\n    base_size = 12\n  ) +\n    theme(\n      panel.grid.minor = element_blank(),\n      panel.grid.major.x = element_blank()\n    )\n)\n\ncol1 <- \"#2980b9\"\ncol2 <- \"#2980b9\"\ndata_path <- here(\"data\", \"cleaned_data.rds\")\nif (file.exists(data_path)) {\n  d <- read_rds(file = data_path)\n} else {\n  stop(str_glue(\"{data_path} doesn't exist, run `01-process.Rmd` to create it.\"))\n}"},{"path":"descriptives.html","id":"demographics-final-sample","chapter":"3 Descriptives","heading":"3.1 Demographics final sample","text":"create table showcases demographic information.\nTable 3.1: Sample demographics\n","code":"\nd %>%\n  filter(wid == 1) %>%\n  distinct(pid, Game, Age, Experience, Gender) %>%\n  select(-pid) %>%\n  tbl_summary(by = Game, missing_text = \"Missing\") %>%\n  add_overall() %>%\n  bold_labels() %>%\n  italicize_levels() -> table1\n\nif (!dir.exists(here(\"output\"))) {\n  dir.create(\"output\", FALSE)\n}\n\nif (!file.exists(here(\"output\", \"table1.docx\"))){\n  table1 %>% \n    as_flex_table() %>% \n    flextable::save_as_docx(path = here(\"output\", \"table1.docx\"))\n}\n\ntable1 %>% \n  as_kable_extra(caption = \"Sample demographics\") %>% \n  kable_styling(full_width = FALSE, font_size = 12)"},{"path":"descriptives.html","id":"descriptives-survey","chapter":"3 Descriptives","heading":"3.2 Descriptives survey","text":"","code":""},{"path":"descriptives.html","id":"retention-rates","chapter":"3 Descriptives","heading":"3.2.1 Retention rates","text":", see many players responded survey calculate retention.\nPlease note number wave 1 participants slightly different previous tables.\nprevious tables, counted anyone telemetry one wave responded one wave (angry item).\n, filtered whether someone responded.\nperson consented participate, didn’t fill anything wave 1, got recontacted filled everything waves 2 3, won’t count towards completed wave 1 one , towards total sample.\nTable 2.1: Number people (response/retention rate) participating wave.\n","code":"\n# Get data on invite dates and Ns\nif (!file.exists(here(\"data\", \"invites.csv\"))){\n  download.file(\"https://osf.io/8uk2x/download\", here(\"data\", \"invites.csv\"), mode = \"wb\" )\n}\n\ninvites <- read_csv(here(\"data\", \"invites.csv\")) %>%\n  rename(Game = game) %>%\n  filter(Game %in% c(\"Apex Legends\", \"Outriders\"))\n\n# Create a table where wave 0 are number of invites,\n# then calculate response rate / retention at each wave.\n# This assumes there are no new participants at wave 3\n# (people who didn't participate in wave 2 showed up at wave 3).\nbind_rows(\n  select(invites, -date),\n  d %>% filter(Responded) %>% count(Game, wid)\n) %>%\n  arrange(Game, wid) -> retention\n\nretention %>%\n  group_by(Game) %>% \n  mutate(\n    R_rate = percent(n / lag(n), .01),\n    n = comma(n)\n  ) %>% \n  pivot_wider(names_from = wid, values_from = c(n, R_rate)) %>%\n  mutate(\n    Invites = n_0,\n    `Wave 1` = str_glue(\"{n_1} ({R_rate_1})\"),\n    `Wave 2` = str_glue(\"{n_2} ({R_rate_2})\"),\n    `Wave 3` = str_glue(\"{n_3} ({R_rate_3})\")\n  ) %>%\n  select(Game, Invites:`Wave 3`) %>%\n  mutate(across(everything(), ~ str_replace(., \"NA\", \"0\"))) %>%\n  kbl(caption = \"Number of people (response/retention rate) participating at each wave.\") %>% \n  kable_styling(full_width = FALSE, font_size = 12)"},{"path":"descriptives.html","id":"response-dates","chapter":"3 Descriptives","heading":"3.2.2 Response dates","text":"plot response dates participants responded anger item.\nsee participants filled wave 3 extremely late.\nreceived negative interval (see processing) thus one wave valid data.","code":"\nd %>%\n  # Take only actually responded-to waves\n  filter(Responded) %>%\n  mutate(Date = as_date(StartDate)) %>%\n  count(Game, Wave, Date) %>%\n  ggplot(\n    aes(Date, n, fill = Wave)\n  ) +\n  geom_col() +\n  scale_y_continuous(\n    \"Responses\",\n    breaks = pretty_breaks(),\n    expand = expansion(c(0, .1)),\n  ) +\n  scale_x_date(\n    \"Date\",\n    date_breaks = \"7 day\", date_labels = \"%b\\n%d\", date_minor_breaks = \"1 day\"\n  ) +\n  facet_wrap(\"Game\", scales = \"free_y\", ncol = 1)"},{"path":"descriptives.html","id":"response-times","chapter":"3 Descriptives","heading":"3.2.3 Response times","text":"see participants filled surveys late evening.","code":"\nd %>%\n  filter(Responded) %>%\n  mutate(Hour = hour(StartDate)) %>%\n  count(Game, Wave, Hour) %>%\n  ggplot(aes(Hour, y = n, fill = Wave)) +\n  scale_y_continuous(\n    \"Responses\",\n    breaks = pretty_breaks(),\n    expand = expansion(c(0, .1)),\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 21, by = 3),\n    expand = expansion(c(0.01))\n  ) +\n  geom_col() +\n  facet_wrap(\"Game\", scales = \"free\", ncol = 2) +\n  theme(legend.position = \"bottom\")"},{"path":"descriptives.html","id":"intervals-between-waves","chapter":"3 Descriptives","heading":"3.2.4 Intervals between waves","text":"partners sent invitations exactly 14 days.\nAlso, participants free take surveys whenever wanted.\nTherefore, interval responses variable.\nMoreover, participant consented participate one wave good data, “filled ” (empty rows) waves, simply added 14 days.Therefore, get actual window (excluding artificial +14 days waves), ’ll filter successive waves.\n, ’ll include “real” surveys (aka Responded variable true).\nfilter either waves 1 2, waves.\nTable 3.2: Interval duration percentiles preceding waves 2 3.\n","code":"\nsuccessive_waves <- \n  d %>% \n  # wide format so we have a column for each wave with a TRUE if there was a response\n  # this way, we can check (manually) whether responses were successive\n  pivot_wider(\n    id_cols = c(pid, Game),\n    names_from = wid,\n    names_glue = \"Wave {wid}\",\n    values_from = Responded\n  ) %>% \n  # proper TRUE/FALSE for whether someone responded\n  mutate(\n    across(\n      `Wave 1`:`Wave 3`,\n      ~ replace_na(., FALSE)\n    )\n  ) %>% \n  # then create a variable indicating whether someone has successive waves\n  mutate(\n    successive = case_when(\n      (`Wave 1` == TRUE & `Wave 2` == TRUE) |\n        (`Wave 2` == TRUE & `Wave 3` == TRUE) |\n        (`Wave 1` == TRUE & `Wave 2` == TRUE & `Wave 3` == TRUE) ~ TRUE,\n      TRUE ~ FALSE\n    )\n  ) %>% \n  filter(successive == TRUE) %>% \n  pull(pid)\n\n# now we just keep those who have successive waves and filter out waves where players didn't respond\nd %>% \n  filter(pid %in% successive_waves, Responded == TRUE) %>% \n  select(Wave, Game, interval) %>%\n  filter(Wave != \"Wave 1\") %>%\n  summarise(\n    Value = quantile(\n      interval,\n      probs = c(0, .10, .25, .5, .75, .90, 1),\n      na.rm = T\n    ) %>%\n      round(3),\n    Quantile = percent(c(0, .10, .25, .5, .75, .90, 1))\n  ) %>%\n  pivot_wider(names_from = Quantile, values_from = Value) %>%\n  kbl(caption = \"Interval duration percentiles preceding waves 2 and 3.\") %>% \n  kable_styling(full_width = FALSE, font_size = 12)\n# Figure\nd %>%\n  filter(pid %in% successive_waves, Responded == TRUE) %>% \n  filter(Wave != \"Wave 1\") %>%\n  filter(between(interval, 5, 30)) %>%\n  mutate(Wave = fct_drop(Wave)) %>%\n  ggplot(aes(interval)) +\n  geom_vline(xintercept = 14, size = .2) +\n  geom_histogram(binwidth = 0.5, col = \"white\") +\n  scale_y_continuous(\n    \"Count\",\n    expand = expansion(c(0, .1))\n  ) +\n  scale_x_continuous(\n    \"Days between responding\",\n    breaks = pretty_breaks()\n  ) +\n  facet_grid(Game ~ Wave, scales = \"free_y\")"},{"path":"descriptives.html","id":"descriptives-telemetry","chapter":"3 Descriptives","heading":"3.3 Descriptives telemetry","text":"Get idea distribution hours played (per day) per game.","code":"\nd %>%\n  ggplot(aes(Hours)) +\n  geom_histogram() +\n  scale_x_continuous(\n    \"Duration (in hours) of individual sessions\",\n    breaks = pretty_breaks()\n  ) +\n  scale_y_continuous(expand = expansion(c(0, .1))) +\n  facet_wrap(\"Game\", scales = \"free\", ncol = 2)"},{"path":"descriptives.html","id":"plots-for-paper","chapter":"3 Descriptives","heading":"3.4 Plots for paper","text":"plots similar (identical) Vuorre et al. (2021).","code":""},{"path":"descriptives.html","id":"figure-0","chapter":"3 Descriptives","heading":"3.4.1 Figure 0","text":"first figure simply show number participants per wave (aka retention).\none isn’t manuscript anymore (hence Figure 0).","code":"\nretention %>%\n  filter(wid != 0) %>% \n  group_by(Game) %>% \n  mutate(`%` = percent(n / max(n), .1),\n         `%` = if_else(is.na(`%`), percent(1, .1), `%`)) %>% \n  ungroup() %>% \n  mutate(Wave = factor(wid)) %>% \n  ggplot(\n    aes(x = Wave, n, fill = Game, )\n  ) +\n  geom_bar(stat = \"identity\", position = position_dodge(0.8), width = 0.8) +\n  geom_text(\n    aes(\n      label = `%`,\n      y = n + max(retention[retention$wid != 0,]$n) * .03\n    ),\n    position = position_dodge(0.8)\n  ) +\n  scale_fill_manual(values = c(\"grey50\", \"black\")) +\n  ylab(\"# of participants\") +\n  theme(\n    legend.position = c(0.8,0.787)\n  )"},{"path":"descriptives.html","id":"figure-1","chapter":"3 Descriptives","heading":"3.4.2 Figure 1","text":"Shows distributions central variables.\nActually, ’ll use one, instead opting beeswarm plot next.paper: Percentage excluded values.\nTable 3.3: Table hours excluded figure\nTry alternative ggbeeswarm can still see individual data points.\n’ll paper.","code":"\ntmp <- d %>%\n  rename(`Aggressive affect` = Angry) %>% \n  select(\n    Game, Wave, pid,\n    `Aggressive affect`, Hours\n  ) %>%\n  pivot_longer(`Aggressive affect`:Hours) %>%\n  drop_na(value) %>%\n  filter(!(name == \"Hours\" & value > 3)) %>%\n  mutate(name = fct_inorder(name))\n\ntmp %>%\n  ggplot(\n    aes(\n      Wave,\n      value,\n      fill = Game,\n      color, Game,\n      group = Game\n    )\n  ) +\n  scale_x_discrete(labels = 1:3, expand = expansion(c(0.1, .1))) +\n  scale_color_manual(\n    values = c(col1, col2, col1, col2, col1, col2, col1),\n    aesthetics = c(\"fill\", \"color\", \"slab_color\")\n  ) +\n  scale_y_continuous(\n    \"Value\",\n    breaks = pretty_breaks(),\n    expand = expansion(.025)\n  ) +\n  geom_blank() +\n  stat_halfeye(\n    alpha = .33,\n    height = .02,\n    normalize = \"panels\",\n    adjust = 1.1,\n    point_interval = NULL,\n    show.legend = FALSE\n  ) +\n  # only the outline (hackish)\n  stat_halfeye(\n    alpha = 1,\n    height = .02,\n    normalize = \"panels\",\n    aes(slab_color = Game),\n    slab_size = 0.5,\n    fill = NA,\n    adjust = 1.1,\n    point_interval = NULL,\n    show.legend = FALSE\n  ) +\n  stat_summary(\n    fun.data = mean_cl_normal,\n    fatten = 1.25\n  ) +\n  stat_summary(\n    fun = mean,\n    geom = \"line\",\n    size = .33\n  ) +\n  facet_grid(name ~ Game, scales = \"free_y\") +\n  theme(\n    legend.position = \"none\"\n    # strip.text.x = element_text(size = 12)\n  )\nd %>%\n  mutate(Hours_over_3 = Hours > 3) %>%\n  tabyl(Hours_over_3) %>%\n  adorn_pct_formatting() %>% \n  kbl(caption = \"Table of hours excluded from figure\") %>% \n  kable_styling(full_width = FALSE, font_size = 12)\ntmp_summary <- \n  tmp %>% \n  group_by(name, Game, Wave) %>% \n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    se = sqrt(var(value)/length(value)),\n    n = n(),\n    lower_ci = mean - 1.96*se,\n    upper_ci = mean + 1.96*se\n  )\n\ntmp %>% \n  ggplot(\n    aes(Wave, value)\n  ) +\n  scale_x_discrete(labels = 1:3) +\n  geom_quasirandom(color = \"grey52\") +\n  geom_point(\n    data = tmp_summary,\n    aes(y = mean),\n    size = 2\n  ) + \n  geom_errorbar(\n    data = tmp_summary,\n    aes(\n      y = mean,\n      ymin = lower_ci,\n      ymax = upper_ci\n    ),\n    width = 0\n  ) +\n  geom_line(\n    data = tmp_summary, \n    aes(y = mean, group = 1),\n    size = 0.7\n  ) +\n  ylab(\"Value\") +\n  facet_grid(\n    name ~ Game,\n    scales = \"free_y\"\n  )"},{"path":"descriptives.html","id":"figure-2","chapter":"3 Descriptives","heading":"3.4.3 Figure 2","text":"’ll show simple bivariate correlations hours played feelings anger wave game.purposes paper, cross-lagged scatter associations probably informative, ’ll use figure .","code":"\nd %>%\n  group_by(Game, Wave) %>%\n  summarise(\n    tidy(\n      lm(\n        Angry ~ Hours, \n        data = cur_data()\n      ), \n      conf.int = TRUE\n    ),\n  ) %>%\n  filter(term != \"(Intercept)\") %>%\n  ggplot(aes(estimate, Game, col = Wave)) +\n  geom_vline(xintercept = 0, lty = 2, size = .25) +\n  scale_x_continuous(\n    \"Bivariate regression coefficient (95%CI)\",\n    breaks = pretty_breaks()\n  ) +\n  geom_pointrange(\n    aes(xmin = conf.low, xmax = conf.high),\n    size = 0.8,\n    position = position_dodge2v(.25)\n  ) +\n  xlim(-0.4, 0.4) +\n  theme(\n    legend.position = \"bottom\",\n    axis.title.y = element_blank()\n  )\nd %>%\n  mutate(Hours = if_else(Hours > 3, NaN, Hours)) %>%\n  mutate(Hours = lag(Hours)) %>% \n  filter(Wave != \"Wave 1\") %>% \n  ggplot(aes(Hours, Angry)) +\n  scale_x_continuous(\n    \"Hours played per day at previous wave\",\n    breaks = pretty_breaks(3)\n  ) +\n  scale_y_continuous(\n    \"Aggressive affect at current wave\",\n    breaks = pretty_breaks()\n  ) +\n  geom_point(size = .2, alpha = .2, shape = 1, colour = \"black\") +\n  geom_smooth(\n    method = \"gam\", size = .4,\n    color = \"black\",\n    alpha = .33, show.legend = FALSE\n  ) +\n  facet_grid(\n    Wave ~ Game,\n    scales = \"free_y\"\n  ) +\n  theme(\n    aspect.ratio = 1,\n    panel.grid = element_blank(),\n    strip.text.x = element_text(size = 10)\n  )"},{"path":"analysis.html","id":"analysis","chapter":"4 Analysis","heading":"4 Analysis","text":"Set figure options like .Load cleaned data set previous section.","code":"\nlibrary(lavaan)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(showtext)\nlibrary(ggstance)\nlibrary(extrafont)\nlibrary(kableExtra)\n# Plotting options\nFont <- \"Titillium Web\"\nfont_add_google(Font, Font)\ntheme_set(\n  theme_linedraw(\n    base_family = Font,\n    base_size = 12\n  ) +\n    theme(\n      panel.grid.minor = element_blank(),\n      panel.grid.major.x = element_blank()\n    )\n)\n\ncol1 <- \"#2980b9\"\ncol2 <- \"#2980b9\"\ndata_path <- here(\"data\", \"cleaned_data.rds\")\nif (file.exists(data_path)) {\n  d <- read_rds(file = data_path)\n} else {\n  stop(str_glue(\"{data_path} doesn't exist, run `01-process.Rmd` to create it.\"))\n}"},{"path":"analysis.html","id":"run-model","chapter":"4 Analysis","heading":"4.1 Run model","text":"syntax lavaan.\nNote constrain cross-lagged effects , within game (effects can different games).\nSee explanation : https://lavaan.ugent./tutorial/groups.htmlTransform data wide format lavaan take arguments .Fitting model.Inspecting summary.Get parameter estimates plot.\nNote: standardized estimates aren’t across waves didn’t put equality constraints variances – unstandardized ones identical.\nTable 3.2: Parameter RICLPM\n","code":"\nriclpm_constrained <- [1058 chars quoted with '\"']\nd_riclpm <- d %>%\n  select(\n    Game, pid, wid,\n    Hours, Angry\n  ) %>%\n  # Long format on anger (outcome)\n  pivot_longer(\n    Angry,\n    names_to = \"y_var\", values_to = \"y\"\n  ) %>%\n  # Long format on hours (predictor)\n  pivot_longer(\n    Hours,\n    names_to = \"x_var\", values_to = \"x\"\n  ) %>% \n  pivot_wider(\n    names_from = wid,\n    values_from = c(x,y),\n    names_sep = \"\"\n  )\nlavaan_fit <- lavaan(\n  riclpm_constrained,\n  data = d_riclpm,\n  missing = \"ml\",\n  meanstructure = TRUE,\n  int.ov.free = TRUE,\n  group = \"Game\"\n)\nsummary(lavaan_fit)## lavaan 0.6-9 ended normally after 95 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                        52\n##   Number of equality constraints                     8\n##                                                       \n##   Number of observations per group:                   \n##     Apex Legends                                  1092\n##     Outriders                                     1488\n##   Number of missing patterns per group:               \n##     Apex Legends                                     8\n##     Outriders                                        7\n##                                                       \n## Model Test User Model:\n##                                                       \n##   Test statistic                                55.775\n##   Degrees of freedom                                10\n##   P-value (Chi-square)                           0.000\n##   Test statistic for each group:\n##     Apex Legends                                 3.680\n##     Outriders                                   52.095\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Observed\n##   Observed information based on                Hessian\n## \n## \n## Group 1 [Apex Legends]:\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   RIx =~                                              \n##     x1                1.000                           \n##     x2                1.000                           \n##     x3                1.000                           \n##   RIy =~                                              \n##     y1                1.000                           \n##     y2                1.000                           \n##     y3                1.000                           \n##   wx1 =~                                              \n##     x1                1.000                           \n##   wx2 =~                                              \n##     x2                1.000                           \n##   wx3 =~                                              \n##     x3                1.000                           \n##   wy1 =~                                              \n##     y1                1.000                           \n##   wy2 =~                                              \n##     y2                1.000                           \n##   wy3 =~                                              \n##     y3                1.000                           \n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   wx2 ~                                               \n##     wx1      (bxa)    0.457    0.053    8.677    0.000\n##     wy1      (gxa)    0.027    0.025    1.113    0.266\n##   wy2 ~                                               \n##     wx1      (gya)   -0.008    0.168   -0.048    0.961\n##     wy1      (bya)    0.040    0.128    0.310    0.756\n##   wx3 ~                                               \n##     wx2      (bxa)    0.457    0.053    8.677    0.000\n##     wy2      (gxa)    0.027    0.025    1.113    0.266\n##   wy3 ~                                               \n##     wx2      (gya)   -0.008    0.168   -0.048    0.961\n##     wy2      (bya)    0.040    0.128    0.310    0.756\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   wx1 ~~                                              \n##     wy1               0.052    0.045    1.158    0.247\n##  .wx2 ~~                                              \n##    .wy2               0.018    0.046    0.396    0.692\n##  .wx3 ~~                                              \n##    .wy3               0.048    0.040    1.209    0.227\n##   RIx ~~                                              \n##     RIy               0.011    0.048    0.232    0.816\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##    .x1                0.765    0.025   30.549    0.000\n##    .x2                0.671    0.026   26.141    0.000\n##    .x3                0.670    0.025   26.777    0.000\n##    .y1                3.422    0.048   71.715    0.000\n##    .y2                3.237    0.069   46.638    0.000\n##    .y3                3.235    0.092   35.062    0.000\n##     RIx               0.000                           \n##     RIy               0.000                           \n##     wx1               0.000                           \n##    .wx2               0.000                           \n##    .wx3               0.000                           \n##     wy1               0.000                           \n##    .wy2               0.000                           \n##    .wy3               0.000                           \n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##     RIx               0.386    0.035   11.145    0.000\n##     RIy               1.300    0.155    8.388    0.000\n##     wx1               0.298    0.030   10.030    0.000\n##     wy1               1.124    0.150    7.509    0.000\n##    .wx2               0.269    0.017   15.519    0.000\n##    .wy2               1.028    0.186    5.522    0.000\n##    .wx3               0.226    0.013   16.995    0.000\n##    .wy3               1.107    0.151    7.318    0.000\n##    .x1                0.000                           \n##    .x2                0.000                           \n##    .x3                0.000                           \n##    .y1                0.000                           \n##    .y2                0.000                           \n##    .y3                0.000                           \n## \n## \n## Group 2 [Outriders]:\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   RIx =~                                              \n##     x1                1.000                           \n##     x2                1.000                           \n##     x3                1.000                           \n##   RIy =~                                              \n##     y1                1.000                           \n##     y2                1.000                           \n##     y3                1.000                           \n##   wx1 =~                                              \n##     x1                1.000                           \n##   wx2 =~                                              \n##     x2                1.000                           \n##   wx3 =~                                              \n##     x3                1.000                           \n##   wy1 =~                                              \n##     y1                1.000                           \n##   wy2 =~                                              \n##     y2                1.000                           \n##   wy3 =~                                              \n##     y3                1.000                           \n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   wx2 ~                                               \n##     wx1      (bxo)    0.553    0.042   13.091    0.000\n##     wy1      (gxo)    0.016    0.027    0.609    0.543\n##   wy2 ~                                               \n##     wx1      (gyo)   -0.024    0.059   -0.414    0.679\n##     wy1      (byo)    0.025    0.115    0.220    0.826\n##   wx3 ~                                               \n##     wx2      (bxo)    0.553    0.042   13.091    0.000\n##     wy2      (gxo)    0.016    0.027    0.609    0.543\n##   wy3 ~                                               \n##     wx2      (gyo)   -0.024    0.059   -0.414    0.679\n##     wy2      (byo)    0.025    0.115    0.220    0.826\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##   wx1 ~~                                              \n##     wy1               0.078    0.075    1.043    0.297\n##  .wx2 ~~                                              \n##    .wy2              -0.034    0.040   -0.840    0.401\n##  .wx3 ~~                                              \n##    .wy3               0.029    0.028    1.038    0.299\n##   RIx ~~                                              \n##     RIy              -0.042    0.050   -0.838    0.402\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##    .x1                0.666    0.030   22.512    0.000\n##    .x2                0.373    0.023   16.503    0.000\n##    .x3                0.207    0.017   12.372    0.000\n##    .y1                2.831    0.037   76.839    0.000\n##    .y2                2.838    0.059   47.900    0.000\n##    .y3                2.782    0.065   42.942    0.000\n##     RIx               0.000                           \n##     RIy               0.000                           \n##     wx1               0.000                           \n##    .wx2               0.000                           \n##    .wx3               0.000                           \n##     wy1               0.000                           \n##    .wy2               0.000                           \n##    .wy3               0.000                           \n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)\n##     RIx              -0.179    0.108   -1.659    0.097\n##     RIy               0.918    0.102    8.963    0.000\n##     wx1               1.483    0.140   10.570    0.000\n##     wy1               1.097    0.099   11.081    0.000\n##    .wx2               0.485    0.021   23.505    0.000\n##    .wy2               0.750    0.167    4.491    0.000\n##    .wx3               0.310    0.026   11.985    0.000\n##    .wy3               0.937    0.100    9.368    0.000\n##    .x1                0.000                           \n##    .x2                0.000                           \n##    .x3                0.000                           \n##    .y1                0.000                           \n##    .y2                0.000                           \n##    .y3                0.000\nparams <- \n  bind_rows(\n    parameterestimates(lavaan_fit) %>% \n      mutate(Type = \"Unstandardized\"),\n    standardizedsolution(lavaan_fit) %>%\n      rename(est = est.std) %>% \n      mutate(Type = \"Standardized\")\n  ) %>% \n  as_tibble() %>% \n  rename(Game = group) %>% \n  mutate(\n    Game = factor(if_else(Game == 1, \"Apex Legends\", \"Outriders\"))\n  ) %>% \n  mutate(\n    label = if_else(lhs == \"RIx\" & rhs == \"RIy\", \"Covariance\", label)\n  ) %>% \n  mutate(\n    Outcome = case_when(\n      str_starts(lhs, \"wx\") ~ \"Hours\",\n      str_starts(lhs, \"wy\") ~ \"Angry\",\n      TRUE ~ NA_character_\n    ),\n    Predictor = case_when(\n      str_starts(rhs, \"wx\") ~ \"Hours\",\n      str_starts(rhs, \"wy\") ~ \"Angry\",\n      TRUE ~ NA_character_\n    )\n  ) %>%\n  mutate(\n    `Parameter type` = case_when(\n      lhs == \"RIx\" & rhs == \"RIy\" ~ \"Covariance\",\n      Predictor == Outcome ~ \"Autoregression\",\n      Predictor != Outcome ~ \"Cross-lagged\",\n      TRUE ~ NA_character_\n    )\n  ) %>%\n  mutate(\n    Direction = case_when(\n      `Parameter type` == \"Cross-lagged\" ~ str_glue('{Predictor}[plain(\"[t-1]\")]%->%{Outcome}[plain(\"[t]\")]'),\n      TRUE ~ NA_character_\n    ),\n    Direction = as.factor(Direction)\n  ) %>% \n  filter(!label == \"\") %>% \n  mutate(\n    across(\n      c(est, ci.lower, ci.upper),\n      ~ round(.x, digits = 2)\n    )\n  ) %>%\n  select(\n    Predictor,\n    Outcome,\n    `Parameter type`,\n    Direction,\n    Type,\n    Game,\n    Estimate = est,\n    `Lower CI` = ci.lower,\n    `Upper CI` = ci.upper\n  ) %>%\n  distinct() %>% \n  mutate(\n    across(\n      c(Predictor, Outcome, Direction),\n      ~ as.factor(str_replace(.x, \"Angry\", \"`Aggressive affect`\")) # for labelling figure\n    )\n  )\n\nparams %>% \n  select(-Direction) %>% \n  mutate( # temporarily remove backticks (purely cosmetic, for table below)\n    across(\n      c(Predictor, Outcome),\n      ~ str_replace(.x, \"`Aggressive affect`\", \"Aggressive affect\") # for labelling figure\n    )\n  ) %>% \n  kbl(caption = \"Parameter from RICLPM\") %>% \n  kable_styling(full_width = FALSE, font_size = 12) "},{"path":"analysis.html","id":"figure-4","chapter":"4 Analysis","heading":"4.2 Figure 4","text":"forest plot unstandardized cross-lagged effects.","code":"\ntext_estimates <- \n  params %>% \n  filter(Type == \"Unstandardized\", `Parameter type` == \"Cross-lagged\") %>% \n  mutate(\n    label = str_glue(\"{Estimate} [{`Lower CI`}, {`Upper CI`}]\"),\n    x = rep(0.25, 4),\n    y = c(rep(2.1, 2), rep(1.1, 2))\n  )\n\nparams %>%\n  filter(Type == \"Unstandardized\", `Parameter type` == \"Cross-lagged\") %>% \n  mutate(\n    Direction = fct_rev(Direction), # not in alphabetical order\n    Game = fct_rev(Game),\n    estimate_text = str_glue(\"{Estimate} [{`Lower CI`}, {`Upper CI`}]\")\n  ) %>% \n  ggplot(aes(Estimate, Game)) +\n  geom_vline(xintercept = 0, lty = 2, size = .25) +\n  geom_pointrange(\n    aes(xmin = `Lower CI`, xmax = `Upper CI`),\n    size = 0.8\n  ) +\n    scale_x_continuous(\n    breaks = pretty_breaks()\n  ) +\n  xlab(\"Estimated cross-lagged effect [95%CI]\") +\n  xlim(-0.4, 0.4) +\n  facet_wrap(\n    ~ Direction,\n    labeller = labeller(.rows = label_parsed)\n  ) +\n  geom_text(\n    data = text_estimates,\n    mapping = aes(x = x, y = y, label = label)\n  ) +\n  theme(\n    aspect.ratio = 1,\n    axis.title.y = element_blank(),\n    panel.grid.major.y = element_line(color = \"lightgrey\"),\n    \n  )"}]
